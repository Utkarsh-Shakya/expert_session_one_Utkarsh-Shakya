{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.4 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "42ed8e3e7f22d40cdae13fff2ca63bf4c5f9a6091a694ed001eb6db1075f2cae"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Data Preparation\n",
    "\n",
    "### In this notebook we will prepare our data for our search function to use. <br>Currently we have data stored in four different ```csv``` files.<br>\n",
    "* movies.csv\n",
    "* links.csv\n",
    "* ratings.csv\n",
    "* tags.csv\n",
    "<br>\n",
    "### It can be computationally expensive to produce ```analysis results``` from multiple data-sources for incomming stream of requests.<br> So we will prepare our data and save it in an ```easily searchable``` structure."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Import the needed modules...\r\n",
    "import pandas as pd\r\n",
    "from collections import defaultdict\r\n",
    "from os import getcwd"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define Paths to data files."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "PATH_LINKS   = f\"{getcwd()}/dataStore/links.csv\"\r\n",
    "PATH_MOVIES  = f\"{getcwd()}/dataStore/movies.csv\"\r\n",
    "PATH_RATINGS = f\"{getcwd()}/dataStore/ratings.csv\"\r\n",
    "PATH_TAGS    = f\"{getcwd()}/dataStore/tags.csv\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Engineering<br>\n",
    "* ## Get data in dataframes.\n",
    "* ## Convert data to a single dictionary."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\"\r\n",
    "    Read data from movies.csv\r\n",
    "\"\"\"\r\n",
    "df_movies            = pd.read_csv(PATH_MOVIES)\r\n",
    "movies_table_columns = df_movies.columns.tolist()\r\n",
    "print(f\"COLUMNS : {movies_table_columns}\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\"\r\n",
    "    Read data from links.csv\r\n",
    "\"\"\"\r\n",
    "df_links            = pd.read_csv(PATH_LINKS)\r\n",
    "links_table_columns = df_links.columns.tolist()\r\n",
    "print(f\"COLUMNS : {links_table_columns}\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\"\r\n",
    "    Read data from ratings.csv\r\n",
    "\"\"\"\r\n",
    "df_ratings         = pd.read_csv(PATH_RATINGS)\r\n",
    "path_table_columns = df_ratings.columns.tolist()\r\n",
    "print(f\"COLUMNS : {path_table_columns}\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\"\r\n",
    "    Read data from tags.csv\r\n",
    "\"\"\"\r\n",
    "df_tags            = pd.read_csv(PATH_TAGS)\r\n",
    "tags_table_columns = df_tags.columns.tolist()\r\n",
    "print(f\"COLUMNS : {tags_table_columns}\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "* ### ```movieId``` is a common column in all four tables so we will use it as a primary search-key <br>\n",
    "* ### ```userId```  is a common key across two tables, so we will use it as a sort key...\n",
    "* ### A user will always search a movie by its ```title``` so we will create a ```Global secondary index``` to be able to perform search our datastore. <br>it will obviously take some extra space but almost negligible as compared to the size of the original data. <br>In addition, It will make our searching faster and efficient so it's a good deal."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(f\"It is {pd.Series(df_movies['movieId']).is_unique}  that the column 'movieId' has unique values for all entries in movies dataframe.\")\r\n",
    "print(f\"It is {pd.Series(df_links['movieId']).is_unique}  that the column 'movieId' has unique values for all entries in links dataframe.\")\r\n",
    "print(f\"It is {pd.Series(df_ratings['userId']).is_unique} that the column 'userId'  has unique values for all entries in ratings dataframe.\")\r\n",
    "print(f\"It is {pd.Series(df_tags['userId']).is_unique} that the column 'userId'  has unique values for all entries in tags dataframe.\")\r\n",
    "\r\n",
    "# Sort movie dataframe on the basis of movieId as movieId is unique for all entries...\r\n",
    "df_movies_sorted = df_movies.sort_values(by=['movieId'])\r\n",
    "\r\n",
    "# Sort links dataframe on the basis of movieId as movieId is unique for all entries...\r\n",
    "df_links_sorted  = df_links.sort_values(by=['movieId'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# from movies dataframe...\r\n",
    "movieIds    = df_movies_sorted[\"movieId\"].tolist()\r\n",
    "movieTitles = df_movies_sorted[\"title\"].tolist()\r\n",
    "movieGenres = [genre.split(\"|\") for genre in df_movies[\"genres\"].tolist()]\r\n",
    "\r\n",
    "# from links dataframe...\r\n",
    "imdbId  = df_links_sorted[\"imdbId\"].tolist()\r\n",
    "tmdbId  = df_links_sorted[\"tmdbId\"].tolist()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "movieDict             = {}\r\n",
    "global_secondaryIndex = {}\r\n",
    "for idx, movieId in enumerate(movieIds):\r\n",
    "    movieDict[movieId] = {\r\n",
    "        \"genre\" : movieGenres[idx],\r\n",
    "        \"links\" : {\r\n",
    "            \"imdb\" : imdbId[idx], \r\n",
    "            \"tmdb\" : tmdbId[idx]\r\n",
    "        }\r\n",
    "    }\r\n",
    "    \r\n",
    "    global_secondaryIndex[movieTitles[idx]] = movieId"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# delete veriables which are no longer in use while holding large amount of data.\r\n",
    "del movieIds\r\n",
    "del movieTitles\r\n",
    "del movieGenres\r\n",
    "del imdbId\r\n",
    "del tmdbId"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Add all user ratings for individual movies.\n",
    "#### The goal is to group all ratings of a ```movie``` togather, so that we will be able to retrieve user ratings of a particular movie.\n",
    "#### Now, this one is a bit tricky as there is no column in the ratings dataframe which offers unique values. <br>So will have to perform grouping.\n",
    "#### We will use ```movieId``` column as it is a common column in all of our data sources and it will make it easy to add the same data in our  new ```movie``` dataset.\n",
    "\n",
    "#### The procedure defined below may be computationally gross but should be good enough for a single time execution..."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# convert all columns of ratings table into individual lists...\r\n",
    "userIds       = df_ratings[\"userId\"].tolist()\r\n",
    "movieIds      = df_ratings[\"movieId\"].tolist()\r\n",
    "user_ratings  = df_ratings[\"rating\"].tolist()\r\n",
    "timestamps    = df_ratings[\"timestamp\"].tolist()\r\n",
    "\r\n",
    "ratings = {}\r\n",
    "\r\n",
    "for idx, mid in enumerate(movieIds):\r\n",
    "    # Do the movieId previously exist?\r\n",
    "    try   : _ = ratings[mid]\r\n",
    "    # If not, Add it in the record...\r\n",
    "    except: \r\n",
    "        ratings[mid]   = [\r\n",
    "            {\r\n",
    "                \"userId\"     : userIds[idx],\r\n",
    "                \"rating\"     : user_ratings[idx],\r\n",
    "                \"time_stamp\" : timestamps[idx]\r\n",
    "            }\r\n",
    "        ]\r\n",
    "    \r\n",
    "    try   : _ = ratings[mid][userIds[idx]]\r\n",
    "    except: ratings[mid].append(\r\n",
    "            {\r\n",
    "                \"userId\"     : userIds[idx],\r\n",
    "                \"rating\"     : user_ratings[idx],\r\n",
    "                \"time_stamp\" : timestamps[idx]\r\n",
    "            }\r\n",
    "        )\r\n",
    "\r\n",
    "# Finally, add the data in the movieDict...\r\n",
    "for mid, _ in movieDict.items():\r\n",
    "    try   : movieDict[mid][\"user_rating\"] = ratings[mid][1:]\r\n",
    "    except: \r\n",
    "        try   : movieDict[mid][\"user_rating\"] = [] # If Movie ID exists in the movie dict...\r\n",
    "        except: pass # If the Movie ID doesn't exist in our record..."
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Add all user given tags for individual movies.\n",
    "#### The goal is to group all tags given to a ```movie``` togather, so that we will be able to retrieve tags of a particular movie.\n",
    "#### This one is also tricky as there is no column in the tags dataframe which offers unique values. <br>So will have to perform grouping.\n",
    "#### We will use ```movieId``` column as it is a common column in all of our data sources and it will make it easy to add the same data in our  new ```movie``` dataset.\n",
    "\n",
    "#### The procedure defined below may also be computationally gross but should be good enough for a single time execution..."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# convert all columns of ratings table into individual lists...\r\n",
    "userIds    = df_tags[\"userId\"].tolist()\r\n",
    "movieIds   = df_tags[\"movieId\"].tolist()\r\n",
    "user_tag   = df_tags[\"tag\"].tolist()\r\n",
    "timestamps = df_tags[\"timestamp\"].tolist()\r\n",
    "\r\n",
    "tags = {}\r\n",
    "for idx, mid in enumerate(movieIds):\r\n",
    "    # Do the movieId previously exist?\r\n",
    "    try   : _ = tags[mid]\r\n",
    "    # If not, Add it in the record...\r\n",
    "    except: tags[mid] = [\r\n",
    "        {\r\n",
    "            \"userId\"     : userIds[idx],\r\n",
    "            \"rating\"     : user_tag[idx],\r\n",
    "            \"time_stamp\" : timestamps[idx]\r\n",
    "        }\r\n",
    "    ]\r\n",
    "    \r\n",
    "    try   : _ = ratings[mid][userIds[idx]]\r\n",
    "    except: tags[mid].append(\r\n",
    "            {\r\n",
    "                \"userId\"     : userIds[idx],\r\n",
    "                \"rating\"     : user_tag[idx],\r\n",
    "                \"time_stamp\" : timestamps[idx]\r\n",
    "            }\r\n",
    "        )\r\n",
    "\r\n",
    "# Finally, add the data in the movieDict...\r\n",
    "for mid, _ in movieDict.items():\r\n",
    "    try   : movieDict[mid][\"tags\"] = tags[mid][1:]\r\n",
    "    except: \r\n",
    "        try   : movieDict[mid][\"tags\"] = [] # If Movie ID exists in the movie dict...\r\n",
    "        except: del global_secondaryIndex[mid]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import json\r\n",
    "print(\"[INFO] Writing movie Data into the disk...\")\r\n",
    "with open('dataStore/dataFinal.json', 'w') as fp:\r\n",
    "    json.dump(movieDict, fp, sort_keys=True, indent=4)\r\n",
    "print(\"[INFO] Writing Global Secondary Index Data into the disk...\")\r\n",
    "with open('dataStore/dataFinal_GIS.json', 'w') as fp:\r\n",
    "    json.dump(global_secondaryIndex, fp, sort_keys=True, indent=4)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### At this point, our database is ready and it can handel high inflow of requests."
   ],
   "metadata": {}
  }
 ]
}